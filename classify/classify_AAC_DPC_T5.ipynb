{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataAug.tools import dataAugForTestSMOTE, dataAugForTestBorderlineSMOTE\n",
    "from processTools import processRealData\n",
    "import torch\n",
    "from ModelClassify_4 import ModelClassify\n",
    "from targeTools import testThresholdFive, Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "feature_pd = pd.read_csv('/home/kongge/projects/new_protT5/data/AAC_DPC_T5_578.csv')\n",
    "labels_pd = pd.read_csv(\"/home/kongge/projects/new_protT5/data/mutil_label_578.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "            0         1         2         3         4         5         6  \\\n0    0.038674  0.005525  0.027624  0.066298  0.060773  0.077348  0.000000   \n1    0.097744  0.015038  0.067669  0.090226  0.000000  0.060150  0.007519   \n2    0.078212  0.000000  0.039106  0.100559  0.044693  0.050279  0.005587   \n3    0.058824  0.005348  0.037433  0.080214  0.048128  0.016043  0.010695   \n4    0.135870  0.000000  0.059783  0.086957  0.027174  0.038043  0.010870   \n..        ...       ...       ...       ...       ...       ...       ...   \n573  0.063158  0.010526  0.042105  0.068421  0.047368  0.031579  0.010526   \n574  0.051867  0.006224  0.056017  0.093361  0.045643  0.064315  0.004149   \n575  0.045662  0.013699  0.027397  0.027397  0.063927  0.091324  0.027397   \n576  0.095070  0.024648  0.028169  0.028169  0.077465  0.052817  0.024648   \n577  0.065341  0.009943  0.052557  0.066761  0.039773  0.076705  0.019886   \n\n            7         8         9  ...      1434      1435      1436  \\\n0    0.099448  0.044199  0.154696  ...  0.036818  0.026898  0.010529   \n1    0.060150  0.052632  0.150376  ...  0.050693  0.058083  0.040556   \n2    0.100559  0.089385  0.156425  ... -0.017891  0.025206  0.068204   \n3    0.101604  0.085561  0.144385  ... -0.005363  0.065209  0.039654   \n4    0.054348  0.048913  0.119565  ... -0.019232 -0.013705  0.001783   \n..        ...       ...       ...  ...       ...       ...       ...   \n573  0.052632  0.047368  0.089474  ... -0.082709 -0.038805  0.049082   \n574  0.066390  0.064315  0.132780  ...  0.005489 -0.021214 -0.028315   \n575  0.095890  0.027397  0.141553  ... -0.021107  0.007253  0.054399   \n576  0.066901  0.038732  0.102113  ... -0.052751 -0.001265  0.058449   \n577  0.042614  0.071023  0.133523  ... -0.046016  0.012317  0.004503   \n\n         1437      1438      1439      1440      1441      1442      1443  \n0   -0.087857  0.008474  0.021624  0.037671 -0.042609 -0.033987  0.001863  \n1   -0.099527 -0.021974  0.034129 -0.015654 -0.016042  0.018142  0.009868  \n2   -0.120173 -0.041826 -0.001013 -0.015208  0.003502  0.038556  0.032665  \n3   -0.100884  0.034206  0.002743 -0.040962  0.035129  0.013996  0.079778  \n4   -0.069890  0.016761  0.059995 -0.020010 -0.052366 -0.008041 -0.017827  \n..        ...       ...       ...       ...       ...       ...       ...  \n573 -0.072492  0.061679  0.055330 -0.054334  0.022155  0.081683  0.008963  \n574 -0.027093  0.051935  0.024115 -0.000847  0.049707 -0.002306  0.000640  \n575 -0.098169  0.005296  0.016531  0.015466 -0.041228  0.019770 -0.002437  \n576 -0.104947  0.083710  0.017370 -0.021055  0.059610  0.035077  0.055231  \n577 -0.048036  0.045243  0.020221  0.018960  0.008882  0.032990  0.021255  \n\n[578 rows x 1444 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>1434</th>\n      <th>1435</th>\n      <th>1436</th>\n      <th>1437</th>\n      <th>1438</th>\n      <th>1439</th>\n      <th>1440</th>\n      <th>1441</th>\n      <th>1442</th>\n      <th>1443</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.038674</td>\n      <td>0.005525</td>\n      <td>0.027624</td>\n      <td>0.066298</td>\n      <td>0.060773</td>\n      <td>0.077348</td>\n      <td>0.000000</td>\n      <td>0.099448</td>\n      <td>0.044199</td>\n      <td>0.154696</td>\n      <td>...</td>\n      <td>0.036818</td>\n      <td>0.026898</td>\n      <td>0.010529</td>\n      <td>-0.087857</td>\n      <td>0.008474</td>\n      <td>0.021624</td>\n      <td>0.037671</td>\n      <td>-0.042609</td>\n      <td>-0.033987</td>\n      <td>0.001863</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.097744</td>\n      <td>0.015038</td>\n      <td>0.067669</td>\n      <td>0.090226</td>\n      <td>0.000000</td>\n      <td>0.060150</td>\n      <td>0.007519</td>\n      <td>0.060150</td>\n      <td>0.052632</td>\n      <td>0.150376</td>\n      <td>...</td>\n      <td>0.050693</td>\n      <td>0.058083</td>\n      <td>0.040556</td>\n      <td>-0.099527</td>\n      <td>-0.021974</td>\n      <td>0.034129</td>\n      <td>-0.015654</td>\n      <td>-0.016042</td>\n      <td>0.018142</td>\n      <td>0.009868</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.078212</td>\n      <td>0.000000</td>\n      <td>0.039106</td>\n      <td>0.100559</td>\n      <td>0.044693</td>\n      <td>0.050279</td>\n      <td>0.005587</td>\n      <td>0.100559</td>\n      <td>0.089385</td>\n      <td>0.156425</td>\n      <td>...</td>\n      <td>-0.017891</td>\n      <td>0.025206</td>\n      <td>0.068204</td>\n      <td>-0.120173</td>\n      <td>-0.041826</td>\n      <td>-0.001013</td>\n      <td>-0.015208</td>\n      <td>0.003502</td>\n      <td>0.038556</td>\n      <td>0.032665</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.058824</td>\n      <td>0.005348</td>\n      <td>0.037433</td>\n      <td>0.080214</td>\n      <td>0.048128</td>\n      <td>0.016043</td>\n      <td>0.010695</td>\n      <td>0.101604</td>\n      <td>0.085561</td>\n      <td>0.144385</td>\n      <td>...</td>\n      <td>-0.005363</td>\n      <td>0.065209</td>\n      <td>0.039654</td>\n      <td>-0.100884</td>\n      <td>0.034206</td>\n      <td>0.002743</td>\n      <td>-0.040962</td>\n      <td>0.035129</td>\n      <td>0.013996</td>\n      <td>0.079778</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.135870</td>\n      <td>0.000000</td>\n      <td>0.059783</td>\n      <td>0.086957</td>\n      <td>0.027174</td>\n      <td>0.038043</td>\n      <td>0.010870</td>\n      <td>0.054348</td>\n      <td>0.048913</td>\n      <td>0.119565</td>\n      <td>...</td>\n      <td>-0.019232</td>\n      <td>-0.013705</td>\n      <td>0.001783</td>\n      <td>-0.069890</td>\n      <td>0.016761</td>\n      <td>0.059995</td>\n      <td>-0.020010</td>\n      <td>-0.052366</td>\n      <td>-0.008041</td>\n      <td>-0.017827</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>573</th>\n      <td>0.063158</td>\n      <td>0.010526</td>\n      <td>0.042105</td>\n      <td>0.068421</td>\n      <td>0.047368</td>\n      <td>0.031579</td>\n      <td>0.010526</td>\n      <td>0.052632</td>\n      <td>0.047368</td>\n      <td>0.089474</td>\n      <td>...</td>\n      <td>-0.082709</td>\n      <td>-0.038805</td>\n      <td>0.049082</td>\n      <td>-0.072492</td>\n      <td>0.061679</td>\n      <td>0.055330</td>\n      <td>-0.054334</td>\n      <td>0.022155</td>\n      <td>0.081683</td>\n      <td>0.008963</td>\n    </tr>\n    <tr>\n      <th>574</th>\n      <td>0.051867</td>\n      <td>0.006224</td>\n      <td>0.056017</td>\n      <td>0.093361</td>\n      <td>0.045643</td>\n      <td>0.064315</td>\n      <td>0.004149</td>\n      <td>0.066390</td>\n      <td>0.064315</td>\n      <td>0.132780</td>\n      <td>...</td>\n      <td>0.005489</td>\n      <td>-0.021214</td>\n      <td>-0.028315</td>\n      <td>-0.027093</td>\n      <td>0.051935</td>\n      <td>0.024115</td>\n      <td>-0.000847</td>\n      <td>0.049707</td>\n      <td>-0.002306</td>\n      <td>0.000640</td>\n    </tr>\n    <tr>\n      <th>575</th>\n      <td>0.045662</td>\n      <td>0.013699</td>\n      <td>0.027397</td>\n      <td>0.027397</td>\n      <td>0.063927</td>\n      <td>0.091324</td>\n      <td>0.027397</td>\n      <td>0.095890</td>\n      <td>0.027397</td>\n      <td>0.141553</td>\n      <td>...</td>\n      <td>-0.021107</td>\n      <td>0.007253</td>\n      <td>0.054399</td>\n      <td>-0.098169</td>\n      <td>0.005296</td>\n      <td>0.016531</td>\n      <td>0.015466</td>\n      <td>-0.041228</td>\n      <td>0.019770</td>\n      <td>-0.002437</td>\n    </tr>\n    <tr>\n      <th>576</th>\n      <td>0.095070</td>\n      <td>0.024648</td>\n      <td>0.028169</td>\n      <td>0.028169</td>\n      <td>0.077465</td>\n      <td>0.052817</td>\n      <td>0.024648</td>\n      <td>0.066901</td>\n      <td>0.038732</td>\n      <td>0.102113</td>\n      <td>...</td>\n      <td>-0.052751</td>\n      <td>-0.001265</td>\n      <td>0.058449</td>\n      <td>-0.104947</td>\n      <td>0.083710</td>\n      <td>0.017370</td>\n      <td>-0.021055</td>\n      <td>0.059610</td>\n      <td>0.035077</td>\n      <td>0.055231</td>\n    </tr>\n    <tr>\n      <th>577</th>\n      <td>0.065341</td>\n      <td>0.009943</td>\n      <td>0.052557</td>\n      <td>0.066761</td>\n      <td>0.039773</td>\n      <td>0.076705</td>\n      <td>0.019886</td>\n      <td>0.042614</td>\n      <td>0.071023</td>\n      <td>0.133523</td>\n      <td>...</td>\n      <td>-0.046016</td>\n      <td>0.012317</td>\n      <td>0.004503</td>\n      <td>-0.048036</td>\n      <td>0.045243</td>\n      <td>0.020221</td>\n      <td>0.018960</td>\n      <td>0.008882</td>\n      <td>0.032990</td>\n      <td>0.021255</td>\n    </tr>\n  </tbody>\n</table>\n<p>578 rows × 1444 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_pd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataAugForTestSMOTE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_51319/983519133.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0msmote_multiple\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mitem\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m     \u001B[0mG_feature\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mG_label\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdataAugForTestSMOTE\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeature_pd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels_pd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mitem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1444\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m     \u001B[0mR_feature\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mR_label\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprocessRealData\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeature_pd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels_pd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeature_num\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1444\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'dataAugForTestSMOTE' is not defined"
     ]
    }
   ],
   "source": [
    "smote_multiple = {}\n",
    "for item in range(1, 5, 2):\n",
    "    G_feature, G_label = dataAugForTestSMOTE(feature_pd, labels_pd, item, 1444)\n",
    "    R_feature, R_label = processRealData(feature_pd, labels_pd, feature_num=1444)\n",
    "\n",
    "    train_feature = pd.concat([R_feature, G_feature], axis=0)\n",
    "    train_label = pd.concat([R_label, G_label], axis=0)\n",
    "\n",
    "    test_df = pd.read_csv(\"/home/kongge/projects/new_protT5/data/AAC_DPC_T5_test_mutil_label_122_finally.csv\")\n",
    "\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    datasetTrain = TensorDataset(torch.tensor(train_feature.values), torch.tensor(train_label.values))\n",
    "    batch_size = len(train_feature)\n",
    "    shuffle = True\n",
    "    dataloaderTrain = DataLoader(datasetTrain, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    datasetTest = TensorDataset(torch.tensor(test_df.iloc[:,:1444].values), torch.tensor(test_df.iloc[:,-4:].values))\n",
    "    # 创建 DataLoader 对象，设置批量大小和是否打乱数据\n",
    "    batch_size = len(datasetTest)\n",
    "    dataloaderTest = DataLoader(datasetTest, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = ModelClassify()\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    GMList = {}\n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for idx, data in enumerate(dataloaderTrain, 0):\n",
    "            inputs, labels = data\n",
    "            labels = labels.float()\n",
    "            inputs = inputs.float()\n",
    "            out = model(inputs)\n",
    "            loss = criterion(out, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(dataloaderTrain)\n",
    "        threshold = 0.5\n",
    "        labels_cov = torch.where(out > threshold, torch.tensor(1), torch.tensor(0))\n",
    "        print(f\"Epoch [{epoch+1}/{300}], Average Loss: {avg_loss:.4f}, ACC: {Accuracy(labels.int(), labels_cov)}\")\n",
    "        GMScore1 = testThresholdFive(epoch, model, dataloaderTest)\n",
    "        GMList[epoch] = GMScore1\n",
    "\n",
    "    best_key = max(GMList, key=GMList.get)\n",
    "    best_value = GMList[best_key]\n",
    "    smote_multiple[item] = best_value"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "smote_multiple"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
